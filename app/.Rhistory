ggtitle('Count of MTurk Workers by Most Common Topic Classification') +
ylab('Count of MTurk Workers') +
xlab('Topics')
#test different values of K#
iterations <- tibble(k = 1:15) %>%
mutate(
kclust = map(k, ~kmeans(wid_lod, .x)),
glanced = map(kclust, glance))
iterations %>%
unnest(glanced, .drop = TRUE) %>%
ggplot(aes(x = k)) +
geom_line(size = 2, aes(y = tot.withinss, color = 'within SS')) +
geom_line(size = 2, aes(y = betweenss, color = 'between SS')) +
geom_vline(size = 1.5, aes(xintercept = 6)) +
ggtitle('Between vs. Within Sum of Squares Cluster Comparison') +
ylab('Sum of Squares') +
xlab('K (# of Clusters)')
#execute clusters for given value of K#
set.seed(5000)
wid_clusters <- kmeans(wid_lod, 6, iter.max = 10, nstart = 1,
algorithm = c("Hartigan-Wong"), trace=FALSE)
#create summary cluster table and format#
cluster_summary <- as.data.frame(wid_clusters$centers)
cluster_summary <- cluster_summary %>%
mutate(people = c(1225,1602,804,571,1879,1163)) %>%
mutate(names=c('striving_careers',
'early_parents',
'early_retirees',
'family_heads',
'fresh_grads',
'young_professionals'))
#normalize summary fields for consumable view#
cluster_summary <- cluster_summary[,c(13,12,1,2,3,4,5,6,7,8,9,10,11)]
cluster_summary$age <- round(cluster_summary$age, digits = 1)
cluster_summary$parent <- round(cluster_summary$parent, digits = 2)
cluster_summary$single <- round(cluster_summary$single, digits = 2)
cluster_summary$seeingfriends <- round(cluster_summary$seeingfriends, digits = 4)
cluster_summary$household <- round(cluster_summary$household, digits = 4)
cluster_summary$accomplishment <- round(cluster_summary$accomplishment, digits = 4)
cluster_summary$entertainment <- round(cluster_summary$entertainment, digits = 4)
cluster_summary$family <- round(cluster_summary$family, digits = 4)
cluster_summary$time <- round(cluster_summary$time, digits = 4)
cluster_summary$general <- round(cluster_summary$general, digits = 4)
cluster_summary$food <- round(cluster_summary$food, digits = 4)
formattable(cluster_summary,
align = c('l','c','c','c','c','c','c','c','c','c','c','c','c'),
list(`names` = formatter("span", style = ~ style(color = "grey",font.weight = "bold")),
`seeingfriends` = color_tile('#ff7f7f', '#71CA97'),
`household` = color_tile('#ff7f7f', '#71CA97'),
`accomplishment` = color_tile('#ff7f7f', '#71CA97'),
`entertainment` = color_tile('#ff7f7f', '#71CA97'),
`family` = color_tile('#ff7f7f', '#71CA97'),
`time` = color_tile('#ff7f7f', '#71CA97'),
`general` = color_tile('#ff7f7f', '#71CA97'),
`food` = color_tile('#ff7f7f', '#71CA97')))
#write MTurk extract and cluster IDs out#
write.csv(wid_lod, 'C:/Users/mkars/Documents/GitHub/Spring2019-Proj1-mkarsok/output/wid_lod.csv')
#write master moments file out#
write.csv(hmdb, 'C:/Users/mkars/Documents/GitHub/Spring2019-Proj1-mkarsok/output/hmdb.csv')
print(R.version)
library(dplyr)
library(ggplot2)
library(tm)
library(tidyr)
library(proxy)
library(topicmodels)
library(sqldf)
library(matrixStats)
library(ldatuning)
library(tidytext)
library(stringr)
library(tibble)
library(formattable)
library(purrr)
cleaned_hm <- read.csv('https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv', quote = "")
demographic <- read.csv('https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv', quote = "")
#create HMDB central table with appropriate filters and consolidations#
hmdb <- cleaned_hm %>%
inner_join(demographic, by = 'wid') %>%
filter(country == 'USA') %>%
mutate(age = as.numeric(age)) %>%
filter(age >= 18 & age <= 85) %>%
filter(gender %in% c("m", "f")) %>%
mutate(gender = (gender == 'f')*1) %>%
filter(parenthood %in% c("n", "y")) %>%
mutate(parenthood = (parenthood == 'y')*1) %>%
mutate(marital = (marital == 'single')*1) %>%
mutate(id = row_number())
class(demographic$wid)
#create HMDB central table with appropriate filters and consolidations#
hmdb <- cleaned_hm %>%
mutate(wid = as.integer(wid)) %>%
inner_join(demographic, by = 'wid') %>%
filter(country == 'USA') %>%
mutate(age = as.numeric(age)) %>%
filter(age >= 18 & age <= 85) %>%
filter(gender %in% c("m", "f")) %>%
mutate(gender = (gender == 'f')*1) %>%
filter(parenthood %in% c("n", "y")) %>%
mutate(parenthood = (parenthood == 'y')*1) %>%
mutate(marital = (marital == 'single')*1) %>%
mutate(id = row_number())
#create and process a corpus of all moments in scope#
corpus <- VCorpus(VectorSource(tolower(hmdb$cleaned_hm))) %>%
tm_map(removePunctuation )%>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace) %>%
tm_map(removeWords, stopwords('english'))
#stem words within each moment to obtain the core word for lookup#
stem_hmid <- tidy(tm_map(corpus, stemDocument)) %>%
select(text)
#create master list of words in the corpus - this serves as the dictionary reference document to compare the stemmed words against#
stem_words <- tidy(corpus) %>%
select(text) %>%
unnest_tokens(words, text)
#compare the stemmed words to the list of words in the corpus - match the most common word associated with each stem and create a unique list of stems and references words#
word_reference <- stem_hmid %>%
unnest_tokens(stems, text) %>%
bind_cols(stem_words) %>%
anti_join(stop_words, by = c("words" = "word")) %>%
group_by(stems, words) %>%
summarize(freq = n()) %>%
mutate(clean = words[which.max(freq)]) %>%
distinct(stems, clean)
#leverage the cleaned unique words to re-create the happy moments with cleaned words - call the cleaned moments: processed_moment#
processed <- stem_hmid %>%
mutate(id = row_number()) %>%
unnest_tokens(stems, text) %>%
left_join(word_reference, by = 'stems') %>%
filter(!is.na(clean)) %>%
group_by(id) %>%
summarize(processed_moment = str_c(clean, collapse = " "))
#join the processed_moment with the rest of the data available about the moment and assoicated worker - eliminate one word moments for easier processing in the document term matrix - select relevant attributes for next step analysis#
hmdb <- hmdb %>%
inner_join(processed, by = 'id') %>%
filter(num_sentence <= 5) %>%
filter((sapply(strsplit(as.character(processed_moment), " "), length) > 1) == TRUE) %>%
select(hmid, wid, id, processed_moment, num_sentence, predicted_category,
age, marital, parenthood)
#Create a corpus of processed happy moments - leverage this to create a document term matrix for all processed moments#
corpus_processed <- Corpus(VectorSource(hmdb$processed_moment)) #corpus
dtm <- DocumentTermMatrix(corpus_processed) #document term matrix
#enter parameters for LDA model and execute#
set.seed(3000)
lda <- LDA(dtm,
k = 8,
method = 'Gibbs',
control = list(burnin = 1000,
iter = 5000,
thin = 50,
seed = list(1000,1001),
nstart = 2,
best = TRUE))
knitr::opts_chunk$set(echo = TRUE)
# Step 0 - Load all the required libraries
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
library(scales)
library(wordcloud)
install.packages('wordcloud')
install.packages('wordcloud2')
install.packages('gridExtra')
install.packages('ngram')
install.packages('widyr')
install.packages('igraph')
install.packages('ggraph')
knitr::opts_chunk$set(echo = TRUE)
# Step 0 - Load all the required libraries
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
library(scales)
library(wordcloud)
library(wordcloud2)
library(gridExtra)
library(ngram)
library(widyr)
library(ggplot2)
library(igraph)
library(ggraph)
library(dplyr)
library(tidyr)
# Introduce demo_data, combine with hm_data keep the required columns for analysis
hm_data <- read_csv("../output/processed_moments.csv")
setwd(C:/Users/mkars/Documents/GitHub/Spring2019-Proj1-mkarsok/output)
setwd(Users/mkars/Documents/GitHub/Spring2019-Proj1-mkarsok/output)
setwd(C:\Users\mkars\Documents\GitHub\Spring2019-Proj1-mkarsok\output)
getwd()
setwd(C:/Users/mkars/Documents/GitHub/Spring2019-Proj1-mkarsok/output)
# Introduce demo_data, combine with hm_data keep the required columns for analysis
hm_data <- read_csv("C:\Users\mkars\Documents\GitHub\Spring2019-Proj1-mkarsok\output/processed_moments.csv")
# Introduce demo_data, combine with hm_data keep the required columns for analysis
hm_data <- read_csv("\Users\mkars\Documents\GitHub\Spring2019-Proj1-mkarsok\output/processed_moments.csv")
# Introduce demo_data, combine with hm_data keep the required columns for analysis
hm_data <- read_csv("C:/Users/mkars/Documents/GitHub/Spring2019-Proj1-mkarsok/output/processed_moments.csv")
urlfile <- 'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
knitr::opts_chunk$set(echo = TRUE)
# Step 0 - Load all the required libraries
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
library(scales)
library(wordcloud)
library(wordcloud2)
library(gridExtra)
library(ngram)
library(widyr)
library(ggplot2)
library(igraph)
library(ggraph)
library(dplyr)
library(tidyr)
# Introduce demo_data, combine with hm_data keep the required columns for analysis
hm_data <- read_csv("C:/Users/mkars/Documents/GitHub/Spring2019-Proj1-mkarsok/output/processed_moments.csv")
urlfile <- 'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
hm_data <- hm_data[-c(which(as.numeric(hm_data$age)>120),
which(hm_data$age=="prefer not to say")),]
bag_of_words <-  hm_data %>%
unnest_tokens(word, text)
word_count <- bag_of_words %>%
dplyr::count(word, sort = TRUE)
# Produce a word cloud to get the first feeling of what makes different groups of people happy
wordcloud2(data = word_count[1:100,], shape = 'pentagon', size = 0.6, backgroundColor = "lightblue" )
weight_category<-data.frame(lb=c("achievement", "affection", "bonding", "enjoy_the_moment","leisure","nature","exercise"),dt=c(sum(hm_data$predicted_category=="achievement")/nrow(hm_data), sum(hm_data$predicted_category=="affection")/nrow(hm_data), sum(hm_data$predicted_category=="bonding")/nrow(hm_data), sum(hm_data$predicted_category=="enjoy_the_moment")/nrow(hm_data), sum(hm_data$predicted_category=="leisure")/nrow(hm_data), sum(hm_data$predicted_category=="nature")/nrow(hm_data), sum(hm_data$predicted_category=="exercise")/nrow(hm_data)))
labels1 <- paste(weight_category$lb, round(weight_category$dt*100, digits = 2)) # add percents to labels
labels1 <- paste(labels1, "%", sep="") # ad % to labels
pie(weight_category$dt, labels = labels1, main="Weight of Happiness Categories", col = c("yellow","lightblue","red","green","purple","white","blue"))
freq.df <- count(hm_data, predicted_categories = hm_data$predicted_category)
ggplot(freq.df, aes(predicted_categories, n)) +
geom_histogram(stat="identity", color="black", fill="light blue") +
labs(x="happiness category", y="frequency", title="The Frequency of the 7 Happiness Categories of Happiness") +
geom_text(aes(x=predicted_categories, y=freq.df$n, label=freq.df$n, vjust=-0.5))
par(mfrow=c(2,4))
achievement_group <- hm_data[hm_data$predicted_category == "achievement","gender"]
gender1 <- as.numeric(achievement_group$gender == "m")
gender_of_achievement <- data.frame(gender_label=c("male","female"),
gender_count <- c(sum(gender1==1)/nrow(achievement_group),
sum(gender1==0)/nrow(achievement_group)))
pct <- round(gender_of_achievement$gender_count*100)
labels <- paste(gender_of_achievement$gender_label, pct) # add percents to labels
labels <- paste(labels, "%", sep = "") # ad % to labels
pie(gender_of_achievement$gender_count,labels = labels, main = "Achievement By Gender", radius = 0.8)
affection_group <- hm_data[hm_data$predicted_category == "affection","gender"]
gender2 <- as.numeric(affection_group$gender == "m")
gender_of_affection <- data.frame(gender_label=c("male","female"),
gender_count <- c(sum(gender2==1)/nrow(affection_group),
sum(gender2==0)/nrow(affection_group)))
pct <- round(gender_of_affection$gender_count*100)
labels <- paste(gender_of_affection$gender_label, pct) # add percents to labels
labels <- paste(labels, "%", sep = "") # ad % to labels
pie(gender_of_affection$gender_count,labels = labels, main = "Affection By Gender", radius = 0.8)
bonding_group <- hm_data[hm_data$predicted_category == "bonding","gender"]
gender3 <- as.numeric(bonding_group$gender == "m")
gender_of_bonding <- data.frame(gender_label=c("male","female"),
gender_count <- c(sum(gender3==1)/nrow(bonding_group),
sum(gender3==0)/nrow(bonding_group)))
pct <- round(gender_of_bonding$gender_count*100)
labels <- paste(gender_of_bonding$gender_label, pct) # add percents to labels
labels <- paste(labels, "%", sep = "") # ad % to labels
pie(gender_of_bonding$gender_count,labels = labels, main = "Bonding By Gender", radius = 0.8)
enjoy_the_moment_group <- hm_data[hm_data$predicted_category == "enjoy_the_moment","gender"]
gender4 <- as.numeric(enjoy_the_moment_group$gender == "m")
gender_of_enjoy_the_moment <- data.frame(gender_label=c("male","female"),
gender_count <- c(sum(gender4==1)/nrow(enjoy_the_moment_group),
sum(gender4==0)/nrow(enjoy_the_moment_group)))
pct <- round(gender_of_enjoy_the_moment$gender_count*100)
labels <- paste(gender_of_enjoy_the_moment$gender_label, pct) # add percents to labels
labels <- paste(labels, "%", sep = "") # ad % to labels
pie(gender_of_enjoy_the_moment$gender_count,labels = labels, main = "Enjoy the Moment By Gender", radius = 0.8)
leisure_group <- hm_data[hm_data$predicted_category == "leisure","gender"]
gender5 <- as.numeric(leisure_group$gender == "m")
gender_of_leisure <- data.frame(gender_label=c("male","female"),
gender_count <- c(sum(gender5==1)/nrow(leisure_group),
sum(gender5==0)/nrow(leisure_group)))
pct <- round(gender_of_leisure$gender_count*100)
labels <- paste(gender_of_leisure$gender_label, pct) # add percents to labels
labels <- paste(labels, "%", sep = "") # ad % to labels
pie(gender_of_leisure$gender_count,labels = labels, main = "Leisure By Gender", radius = 0.8)
nature_group <- hm_data[hm_data$predicted_category == "nature","gender"]
gender6 <- as.numeric(nature_group$gender == "m")
gender_of_nature <- data.frame(gender_label=c("male","female"),
gender_count <- c(sum(gender6==1)/nrow(nature_group),
sum(gender6==0)/nrow(nature_group)))
pct <- round(gender_of_nature$gender_count*100)
labels <- paste(gender_of_nature$gender_label, pct) # add percents to labels
labels <- paste(labels, "%", sep = "") # ad % to labels
pie(gender_of_nature$gender_count,labels = labels, main = "Nature By Gender", radius = 0.8)
exercise_group <- hm_data[hm_data$predicted_category == "exercise","gender"]
gender7 <- as.numeric(exercise_group$gender == "m")
gender_of_exercise <- data.frame(gender_label=c("male","female"),
gender_count <- c(sum(gender7==1)/nrow(exercise_group),
sum(gender7==0)/nrow(exercise_group)))
pct <- round(gender_of_exercise$gender_count*100)
labels <- paste(gender_of_exercise$gender_label, pct) # add percents to labels
labels <- paste(labels, "%", sep = "") # ad % to labels
pie(gender_of_exercise$gender_count,labels = labels, main = "Exercise By Gender", radius = 0.8)
bag_of_words_achievement <- bag_of_words[bag_of_words$predicted_category == "achievement",]
bag_of_words_affection <- bag_of_words[bag_of_words$predicted_category == "affection",]
word_count_achievement <- bag_of_words_achievement %>%
dplyr::count(word, sort = TRUE)
word_count_affection <- bag_of_words_affection %>%
dplyr::count(word, sort = TRUE)
frequency_achievement <- bag_of_words_achievement %>%
group_by(gender) %>%
dplyr::count(word, sort = TRUE) %>%
left_join(bag_of_words_achievement %>%
group_by(gender) %>%
summarise(total = n())) %>%
mutate(freq = n/total)
frequency_achievement<- frequency_achievement %>%
select(gender, word, freq) %>%
spread(gender, freq) %>%
arrange(f,m)
ggplot(frequency_achievement, aes(f,m)) +
geom_jitter(alpha = 0.1,color= "skyblue", size = 2, width = 0.2, height = 0.2) +
labs(title="Word Frequency for Female and Male (Achievement)", x="Female", y="Male")+
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
geom_abline(color = "navy")
frequency_affection <- bag_of_words_affection %>%
group_by(gender) %>%
dplyr::count(word, sort = TRUE) %>%
left_join(bag_of_words_affection %>%
group_by(gender) %>%
summarise(total = n())) %>%
mutate(freq = n/total)
frequency_affection <- frequency_affection %>%
select(gender, word, freq) %>%
spread(gender, freq) %>%
arrange(f,m)
ggplot(frequency_affection, aes(f,m)) +
geom_jitter(alpha = 0.1,color= "orangered", size = 2, width = 0.2, height = 0.2) +
labs(title="Word Frequency for Female and Male (Affection)", x="Female", y="Male")+
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
geom_abline(color = "black")
bag_of_words_single <- bag_of_words[bag_of_words$marital=="single",]
bag_of_words_married <- bag_of_words[bag_of_words$marital=="married",]
word_count_single <- bag_of_words_single %>%
dplyr::count(word, sort = TRUE)
word_count_married <- bag_of_words_married %>%
dplyr::count(word, sort = TRUE)
wordcloud(words = word_count_single$word[1:100],
freq = word_count_single$n[1:100],
random.order=FALSE,colors=brewer.pal(8, "Set1"))
wordcloud(words = word_count_married$word[1:100],
freq = word_count_married$n[1:100],
random.order=FALSE,colors=brewer.pal(8, "Dark2"))
word_pair_single <- bag_of_words_single %>%
pairwise_count(word, wid, sort = TRUE, upper = FALSE)
word_pair_single[1:100,] %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "skyblue3") +
geom_node_point(size = 3) +
geom_node_text(aes(label = name), repel = TRUE,point.padding = unit(0.2, "lines")) +
labs(title="Network for Single People")+
theme_void()
word_pair_married <- bag_of_words_married %>%
pairwise_count(word, wid, sort = TRUE, upper = FALSE)
word_pair_married[1:100,] %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "red3") +
geom_node_point(size = 3) +
geom_node_text(aes(label = name), repel = TRUE,point.padding = unit(0.2, "lines")) +
labs(title="Network for Married People")+
theme_void()
word_ratios_single <- bag_of_words_single %>%
count(word, predicted_category) %>%
group_by(word) %>%
filter(sum(n) >= 50) %>%
ungroup() %>%
spread(predicted_category, n, fill = 0) %>%
mutate_if(is.numeric, funs((. + 1) / (sum(.) + 1))) %>%
mutate(logratio = log(achievement /affection )) %>%
arrange(desc(logratio))
word_ratios_single %>%
group_by(logratio < 0) %>%
top_n(15, abs(logratio)) %>%
ungroup() %>%
mutate(word = reorder(word, logratio)) %>%
ggplot(aes(word, logratio, fill = logratio < 0)) +
geom_col(alpha=0.8,show.legend = FALSE) +
coord_flip() +
labs(title="Word Usage for Single")+
ylab("log ratio (achievement/affection)") +
scale_fill_discrete(name = "", labels = c("achievement", "affection"))
word_ratios_married <- bag_of_words_married %>%
count(word, predicted_category) %>%
group_by(word) %>%
filter(sum(n) >= 50) %>%
ungroup() %>%
spread(predicted_category, n, fill = 0) %>%
mutate_if(is.numeric, funs((. + 1) / (sum(.) + 1))) %>%
mutate(logratio = log(achievement /affection )) %>%
arrange(desc(logratio))
word_ratios_married %>%
group_by(logratio < 0) %>%
top_n(15, abs(logratio)) %>%
ungroup() %>%
mutate(word = reorder(word, logratio)) %>%
ggplot(aes(word, logratio, fill = logratio < 0)) +
geom_col(alpha=0.8,show.legend = FALSE) +
coord_flip() +
labs(title="Word Usage for Married")+
ylab("log ratio (achievement/affection)") +
scale_fill_discrete(name = "", labels = c("achievement", "affection"))
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
#load package and data
library("readr")
library("rvest")
library("tibble")
#library("qdap")
library("sentimentr")
shiny::runApp('GitHub/Spring2019-Proj2-grp3/app')
runApp('GitHub/Spring2019-Proj2-grp3/app')
shiny::runApp('GitHub/Spring2019-Proj2-grp3/app')
runApp('GitHub/Spring2019-Proj2-grp3/app')
runApp('GitHub/Spring2019-Proj2-grp3/app')
shiny::runApp('GitHub/Spring2019-Proj2-grp3/app')
runApp('GitHub/Spring2019-Proj2-grp3/app')
getwd()
runApp('GitHub/Spring2019-Proj2-grp3/app')
runApp('GitHub/Spring2019-Proj2-grp3/app')
runApp('GitHub/Spring2019-Proj2-grp3/app')
getwd
getwd()
setwd('C:/Users/mkars/Documents/GitHub/Spring2019-Proj2-grp3/app')
runApp()
getwd()
runApp()
shiny::runApp()
runApp()
runApp()
#set working directory and load finaldata#
setwd('C:/Users/mkars/Documents/GitHub/Spring2019-Proj2-grp3/app')
#load restaurant violation data#
load(file = 'finaldata.RData')
finaldata$zip <- str_sub(finaldata$ADDRESSS, start = -5)
finaldata$mmyy <- paste(INSPECTION.MONTH,INSPECTION.YEAR, sep = "-01-")
colnames(finaldata)
runApp()
colnames(finaldata)
smry_trends <-
finaldata %>%
filter(!is.na(GRADE.DATE) & GRADE %in% c('A', 'B', 'C') &
INSPECTION.YEAR %in% c(2013,2014,2015,2016)) %>%
group_by(mmyy, GRADE) %>%
summarise(CNT = n())
View(smry_trends)
smry_trends <-
finaldata %>%
filter(!is.na(GRADE.DATE) & GRADE %in% c('A', 'B', 'C') &
INSPECTION.YEAR %in% c(2013,2014,2015,2016)) %>%
group_by(mmyy, GRADE) %>%
summarise(CNT = n())
colnames(smry_trends) <- c('date', 'GRADE', 'CNT')
smry_trends$date <- as.Date(smry_trends$date, '%m-%d-%Y')
View(smry_trends)
class(smry_trends)
smry_trends <- as.data.frame(smry_trends)
runApp()
